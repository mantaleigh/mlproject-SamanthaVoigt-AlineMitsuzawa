{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "airbnb_files = ['data/raw_data/Austin_listings.csv', 'data/raw_data/Boston_listings.csv', 'data/raw_data/Asheville_listings.csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def drop_airbnb_cols(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df.drop(['id', 'listing_url', 'scrape_id', 'last_scraped', 'thumbnail_url', 'medium_url', 'picture_url', 'xl_picture_url', 'host_id', 'host_url', 'host_thumbnail_url', 'host_picture_url', 'calendar_last_scraped', 'weekly_price', 'monthly_price', 'neighbourhood_cleansed', 'license', 'jurisdiction_names'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_col_names(files):\n",
    "    counts = {}\n",
    "    \n",
    "    for f in files: \n",
    "        for c in drop_airbnb_cols(f).columns: \n",
    "            if c in counts: \n",
    "                counts[c] += 1\n",
    "            else: \n",
    "                counts[c] = 1\n",
    "\n",
    "    cols = []\n",
    "    for c in counts: \n",
    "        if counts[c] == 3:\n",
    "            cols.append(c)\n",
    "    \n",
    "    return cols\n",
    "\n",
    "def segment(df, train, dev, test): \n",
    "    count = len(df.index)\n",
    "    test = test.append(df[-1*int(count*.1):])\n",
    "    dev = test.append(df[-1*int(count*.2):-1*int(count*.1)])\n",
    "    train = test.append(df[:int(count*.8)])\n",
    "    \n",
    "    return train, dev, test\n",
    "    \n",
    "def featurize(df):\n",
    "    \n",
    "    df['price'] = df['price'].map(lambda x: x.replace('$', \"\").replace(',',\"\"))\n",
    "    df['price'] = df['price'].apply(pd.to_numeric) # turn the price col into a number col\n",
    "    text_cols = df.select_dtypes(exclude=['float64', 'int64'])\n",
    "    num_cols = df.select_dtypes(include=['float64', 'int64'])\n",
    "    \n",
    "    for col in text_cols: \n",
    "        corpus = df[col].fillna(value=\"\").values\n",
    "        print corpus\n",
    "        vectorizer = CountVectorizer()\n",
    "        X = vectorizer.fit_transform(corpus)\n",
    "        print X\n",
    "\n",
    "    \n",
    "    \n",
    "    # call dictvectorizor on that list --> numpy array of features\n",
    "    # convert rest of df into numpy array with price as first value\n",
    "    # append 2 numpy arrays by index\n",
    "    \n",
    "    # return big numpy array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_datasets():\n",
    "    col_names = get_col_names(airbnb_files)\n",
    "    \n",
    "    train = pd.DataFrame(columns = col_names)\n",
    "    dev = pd.DataFrame(columns = col_names)\n",
    "    test = pd.DataFrame(columns = col_names)\n",
    "    \n",
    "    for f in airbnb_files:   \n",
    "        df = pd.read_csv(f)\n",
    "        df = df[col_names]\n",
    "        featurize(df)\n",
    "        train, dev, test = segment(df, train, dev, test) # put data from f into train, dev, test\n",
    "        \n",
    "    return train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2011-03-21' '' '' ..., '2015-05-19' '' '']\n",
      "  (0, 28)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 23)\t1\n",
      "  (7, 33)\t1\n",
      "  (7, 9)\t1\n",
      "  (7, 27)\t1\n",
      "  (9, 15)\t1\n",
      "  (9, 27)\t1\n",
      "  (9, 2)\t1\n",
      "  (11, 13)\t1\n",
      "  (11, 26)\t1\n",
      "  (11, 9)\t1\n",
      "  (12, 14)\t1\n",
      "  (12, 7)\t1\n",
      "  (12, 27)\t1\n",
      "  (13, 11)\t1\n",
      "  (13, 4)\t1\n",
      "  (13, 26)\t1\n",
      "  (14, 16)\t1\n",
      "  (14, 24)\t1\n",
      "  (14, 2)\t1\n",
      "  (16, 6)\t1\n",
      "  (16, 5)\t1\n",
      "  (16, 27)\t1\n",
      "  (19, 17)\t1\n",
      "  :\t:\n",
      "  (5822, 2)\t1\n",
      "  (5823, 6)\t1\n",
      "  (5823, 5)\t1\n",
      "  (5823, 27)\t1\n",
      "  (5824, 38)\t1\n",
      "  (5824, 7)\t1\n",
      "  (5824, 27)\t1\n",
      "  (5826, 29)\t1\n",
      "  (5826, 8)\t1\n",
      "  (5826, 26)\t1\n",
      "  (5827, 16)\t1\n",
      "  (5827, 26)\t1\n",
      "  (5827, 2)\t1\n",
      "  (5828, 16)\t1\n",
      "  (5828, 27)\t1\n",
      "  (5828, 2)\t1\n",
      "  (5829, 8)\t1\n",
      "  (5829, 4)\t1\n",
      "  (5829, 27)\t1\n",
      "  (5830, 35)\t1\n",
      "  (5830, 3)\t1\n",
      "  (5830, 27)\t1\n",
      "  (5832, 18)\t1\n",
      "  (5832, 4)\t1\n",
      "  (5832, 27)\t1\n",
      "['f' 't' 't' ..., 'f' 't' 't']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-bc045be03782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-174-474b0dab09e6>\u001b[0m in \u001b[0;36mcreate_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mfeaturize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# put data from f into train, dev, test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-173-c0a4a633ea2e>\u001b[0m in \u001b[0;36mfeaturize\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samanthavoigt/Documents/Wellesley/ML/venv/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samanthavoigt/Documents/Wellesley/ML/venv/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    782\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "train, dev, test = create_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
